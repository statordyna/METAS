{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "prepared-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "injured-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "from operator import truediv, mul\n",
    "import array\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sealed-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "figured-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(map_data, long=False):\n",
    "    v = []\n",
    "    area = []\n",
    "    ent = []\n",
    "    cnt = 0\n",
    "    for i in map_data.item()['time']:\n",
    "        v.append(float(i.to_sec()))\n",
    "        if(map_data.item()['area'][cnt] != 0):\n",
    "            area.append(map_data.item()['area'][cnt])\n",
    "        else:\n",
    "            area.append(0.0025)\n",
    "        ent.append(map_data.item()['total_entropy'][cnt])\n",
    "        cnt += 1\n",
    "    d = pd.DataFrame(columns=['ts','area','ent'])\n",
    "\n",
    "    d.loc[:,'ts'] = v\n",
    "    d.loc[:,'area'] = area\n",
    "    d.loc[:,'ent'] = ent\n",
    "    d = d.set_index('ts')\n",
    "    d.index = pd.to_datetime(d.index, unit='s')\n",
    "    \n",
    "    tmp = d.resample('2S')[['area']].agg(lambda x : np.nan if x.count() == 0 else x.idxmax()).ffill()\n",
    "\n",
    "    df = d.loc[tmp['area']]\n",
    "    cnt = 0\n",
    "    if (not long):\n",
    "        while (len(df) < 300):\n",
    "            new_row = pd.DataFrame(columns=['area','ent'])\n",
    "            new_row.loc[0 + cnt / 1000.0,'area'] = 0.0025\n",
    "            new_row.loc[0 + cnt / 1000.0,'ent'] = 1\n",
    "            cnt += 1\n",
    "            df = pd.concat([new_row, df])\n",
    "    else:\n",
    "        while (len(df) < 450):\n",
    "            new_row = pd.DataFrame(columns=['area','ent'])\n",
    "            new_row.loc[0 + cnt / 1000.0,'area'] = 0.0025\n",
    "            new_row.loc[0 + cnt / 1000.0,'ent'] = 1\n",
    "            cnt += 1\n",
    "            df = pd.concat([new_row, df])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "large-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#Author   : Dr. Arun B Ayyar\n",
    "#\n",
    "#Based on : Shimazaki H. and Shinomoto S., A method for selecting the bin size of a time histogram Neural Computation (2007)\n",
    "#\t   Vol. 19(6), 1503-1527\n",
    "#\n",
    "#Data     : The duration for eruptions of the Old Faithful geyser in Yellowstone National Park (in minutes) \n",
    "#\t   or normal distribuition.\n",
    "#\t   given at http://176.32.89.45/~hideaki/res/histogram.html\n",
    "#\n",
    "#Comments : Implements a faster version than using hist from matplotlib and histogram from numpy libraries\t\n",
    "#           Also implements the shifts for the bin edges\n",
    "#\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def compute_bin_size(data):\n",
    "    data_max = max(data) #lower end of data\n",
    "    data_min = min(data) #upper end of data\n",
    "    n_min = 2500   #Minimum number of bins Ideal value = 2\n",
    "    n_max = 2501  #Maximum number of bins  Ideal value =200\n",
    "    n_shift = 1     #number of shifts Ideal value = 30\n",
    "    N = np.array(range(n_min,n_max))\n",
    "    D = float(data_max-data_min)/N    #Bin width vector\n",
    "    Cs = np.zeros((len(D),n_shift)) #Cost function vector\n",
    "    #Computation of the cost function\n",
    "    for i in range(np.size(N)):\n",
    "        shift = np.linspace(0,D[i],n_shift)\n",
    "        for j in range(n_shift):\n",
    "            edges = np.linspace(data_min+shift[j]-D[i]/2,data_max+shift[j]-D[i]/2,N[i]+1) # shift the Bin edges\n",
    "            binindex = np.digitize(data,edges) #Find binindex of each data point\n",
    "            ki= np.bincount(binindex)[1:N[i]+1] #Find number of points in each bin\n",
    "            k = np.mean(ki) #Mean of event count\n",
    "            v = sum((ki-k)**2)/N[i] #Variance of event count\n",
    "            Cs[i,j]+= (2*k-v)/((D[i])**2) #The cost Function\n",
    "    C=Cs.mean(1)\n",
    "\n",
    "    #Optimal Bin Size Selection\n",
    "    loc = np.argwhere(Cs==Cs.min())[0]\n",
    "    cmin = C.min()\n",
    "    idx  = np.where(C==cmin)\n",
    "    idx = idx[0][0]\n",
    "    optD = D[idx]\n",
    "    return np.linspace(data_min+shift[loc[1]]-D[idx]/2,data_max+shift[loc[1]]-D[idx]/2,N[idx]+1,endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fleet-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    for l in range(len(oc)):\n",
    "        if(oc[l] == -1):\n",
    "            oc[l] = 50\n",
    "        if(oc_gt[l] == -1):\n",
    "            oc_gt[l] = 50\n",
    "        se += (oc[l] - oc_gt[l])**2\n",
    "    mse = se/len(oc)\n",
    "    return mse**0.5\n",
    "\n",
    "def get_rmse_known(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    cnt = 0.0\n",
    "    for l in range(len(oc)):\n",
    "        if (oc[l] == -1):\n",
    "            if (oc_gt[l] != -1):\n",
    "                oc[l] = 50\n",
    "                cnt += 1\n",
    "                se += (oc[l] - oc_gt[l])**2\n",
    "        else:\n",
    "            if(oc_gt == -1):\n",
    "                oc_gt[l] = 100 - oc[l]\n",
    "            cnt += 1\n",
    "            se += (oc[l] - oc_gt[l])**2\n",
    "    mse = se/cnt\n",
    "    return mse**0.5\n",
    "\n",
    "def bac_manual(occupancy, gt):\n",
    "    oc = []\n",
    "    se = 0\n",
    "    for l in occupancy:\n",
    "        p = float(l)\n",
    "        oc.append(p)\n",
    "    oc_gt = []\n",
    "    for l in gt:\n",
    "        p = float(l)\n",
    "        oc_gt.append(p)\n",
    "    tot_free = 0\n",
    "    tot_occ = 0\n",
    "    corr_free = 0\n",
    "    corr_occ = 0\n",
    "    tot_unk = 0\n",
    "    corr_unk = 0\n",
    "    for l in range(len(oc)):\n",
    "        if oc_gt[l] == 0:\n",
    "            tot_free += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_free += 1\n",
    "        elif oc_gt[l] == 100:\n",
    "            tot_occ += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_occ += 1\n",
    "        elif oc_gt[l] == -1:\n",
    "            tot_unk += 1\n",
    "            if oc[l] == oc_gt[l]:\n",
    "                corr_unk += 1\n",
    "    return [1/3.0*(corr_free/tot_free + corr_occ/tot_occ + corr_unk/tot_unk),corr_free/tot_free,corr_occ/tot_occ, corr_unk/tot_unk, 1/2.0*(corr_free/tot_free + corr_occ/tot_occ)]\n",
    "    # return [1/2.0*(corr_free/tot_free + corr_occ/tot_occ),corr_free/tot_free,corr_occ/tot_occ, corr_unk/tot_unk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "official-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder, columns, kind, df, low = 0, r = 10, ate = False, saving_results = False):\n",
    "    color = ['tab:blue','tab:green','tab:red','tab:orange','tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan','k']\n",
    "    final_folders = []\n",
    "    for k in kind:\n",
    "        final_folders.append(os.path.join(folder, k))\n",
    "\n",
    "    print(final_folders)\n",
    "\n",
    "    cnt = 0\n",
    "    \n",
    "#     fig1, (ax1, ax2) = plt.subplots(1,2)    \n",
    "    fig_map, (ax_map) = plt.subplots(1)\n",
    "    fig_exp, (ax_exp) = plt.subplots(1)\n",
    "#     fig_ent, (ax_ent) = plt.subplots(1)\n",
    "#     fig_area, (ax_area) = plt.subplots(1)\n",
    "    font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 18,\n",
    "            }\n",
    "    ax_exp.set_ylabel('explored %', fontdict=font)\n",
    "    ax_exp.set_xlabel('trajectory length [m]', fontdict=font)\n",
    "    ax_map_split = ax_map.twinx()\n",
    "    ax_map_split.set_ylabel('trajectory length [m]', fontdict=font)\n",
    "    ax_map_split.set_ylim([0,120])\n",
    "\n",
    "    sec_y = ax_exp.secondary_yaxis('right')\n",
    "    sec_y.set_ylabel('normalized entropy', fontdict=font)\n",
    "    sec_y.set_ylim([0,1])\n",
    "    ax_map.set_ylabel('explored %', fontdict=font)\n",
    "    ax_map.set_xlabel('time [s]', fontdict=font)\n",
    "    ax_map.set_ylim([-0.050,1.050])\n",
    "    \n",
    "    for i in final_folders:\n",
    "        tot_score = []\n",
    "        tot_BAC =  []\n",
    "        tot_BAC_free = []\n",
    "        tot_BAC_occ = []\n",
    "        if ate:\n",
    "            tot_TRPE = []\n",
    "            tot_RRPE = []\n",
    "            tot_ATE = []\n",
    "        cov_vec = []\n",
    "        tot_lc = []\n",
    "        glob_lc = []\n",
    "        loc_lc = []\n",
    "        tot_area = []\n",
    "        area_bucketed = []\n",
    "        entropy_bucketed = []\n",
    "        pose_cov_vec = []\n",
    "        final_map_rmse = []\n",
    "        bac_manually = []\n",
    "        bac_manually_unk = []\n",
    "        bac_manually_free = []\n",
    "        bac_manually_occ = []\n",
    "        bac_manually_2 = []\n",
    "        path_len =[]\n",
    "        filtered_path_df = []\n",
    "        wheel_tot = []\n",
    "        for j in range(low,r):\n",
    "            current = os.path.join(i,str(j))\n",
    "            if (os.path.isfile(os.path.join(current,'map_data.npy'))):\n",
    "                map_data = np.load(os.path.join(current,'map_data.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                system_data = np.load(os.path.join(current,'feat.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                wheel_data = np.load(os.path.join(current,'wheels.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                general_results = open(os.path.join(current,'general_results.txt'), 'r')\n",
    "\n",
    "                gt = open(os.path.join(folder,'gt_occupancy.txt'), 'r')\n",
    "                occupancy = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                if(\"E1\" in current):\n",
    "                    occ = np.array(occupancy.read().splitlines())\n",
    "                    occ = occ.reshape((621,-1))\n",
    "                    ncelss = 2/0.05\n",
    "                    occ_shift = np.roll(occ, (-int(ncelss),0), axis=(1,0))\n",
    "                    new_occ = occ_shift.reshape(-1)\n",
    "                    occupancy = map(int, new_occ)\n",
    "\n",
    "                # occupancy_prob = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                occupancy_prob = occupancy\n",
    "                # poses = open(os.path.join(current,'poses.g2o'), 'r')\n",
    "                tot_lc.append(system_data.item()['total'][-1])\n",
    "                glob_lc.append(system_data.item()['global'][-1])\n",
    "                loc_lc.append(system_data.item()['local'][-1])\n",
    "                tot_area.append(map_data.item()['area'][-1])\n",
    "                wheel_tot.append(wheel_data.item()['tot_wheel'][-1])\n",
    "\n",
    "\n",
    "                final_map_rmse.append(get_rmse(occupancy,gt))\n",
    "                \n",
    "                gt = open(os.path.join(folder,'gt_occupancy.txt'), 'r')\n",
    "                occupancy = open(os.path.join(current,'occupancy.txt'), 'r')\n",
    "                if(\"E1\" in current):\n",
    "                    occ = np.array(occupancy.read().splitlines())\n",
    "                    occ = occ.reshape((621,-1))\n",
    "                    ncelss = 2/0.05\n",
    "                    occ_shift = np.roll(occ, (-int(ncelss),0), axis=(1,0))\n",
    "                    new_occ = occ_shift.reshape(-1)\n",
    "                    occupancy = map(int, new_occ)\n",
    "\n",
    "                bac_m = bac_manual(occupancy,gt)\n",
    "                bac_manually.append(bac_m[0])\n",
    "                bac_manually_2.append(bac_m[-1])\n",
    "                bac_manually_unk.append(bac_m[-2])\n",
    "                bac_manually_free.append(bac_m[1])\n",
    "                bac_manually_occ.append(bac_m[2])\n",
    "                \n",
    "                # binning the entropies and the area explored wrt 2-seconds time frames\n",
    "                resampled = resampling(map_data, \"_L\" in current)\n",
    "                if (\"_L\" in current):\n",
    "                    area_bucketed.append(resampled[-450:].loc[:,'area'])            \n",
    "                    entropy_sliced = resampled[-450:].loc[:,'ent']       \n",
    "                else:\n",
    "                    area_bucketed.append(resampled[-300:].loc[:,'area'])            \n",
    "                    entropy_sliced = resampled[-300:].loc[:,'ent']       \n",
    "                a = [0.0025 for i in range(len(area_bucketed[-1]))]\n",
    "                res = list(map(truediv, area_bucketed[-1], a))\n",
    "                res2 = list(map(truediv, entropy_sliced, res))\n",
    "                print(res2[-1])\n",
    "                entropy_bucketed.append(res2)\n",
    "\n",
    "                # ATE - slam metric\n",
    "                if ate:\n",
    "                    path_gt = open(os.path.join(current,'rtabmap_odom.txt'),'r')\n",
    "                    prev = [0,0,0]\n",
    "                    first = True\n",
    "                    path_df = pd.DataFrame(columns=['time','len'])\n",
    "                    \n",
    "                    for l in list(path_gt)[1:]:\n",
    "                        p = l.split()\n",
    "                        if first:\n",
    "                            prev = [float(p[1]), float(p[2])]\n",
    "                            path_df.loc[p[0],'time'] = float(p[0])\n",
    "                            path_df.loc[p[0],'len'] = 0\n",
    "                            cum_dist = 0\n",
    "                            first = False\n",
    "                        else:\n",
    "                            cum_dist += ((prev[0]-float(p[1]))**2+(prev[1]-float(p[2]))**2)**0.5\n",
    "                            \n",
    "                            path_df.loc[p[0],'time'] = float(p[0])\n",
    "                            path_df.loc[p[0],'len'] = cum_dist\n",
    "                            \n",
    "                            prev = [float(p[1]), float(p[2])]\n",
    "                            \n",
    "                    path_len.append(cum_dist)\n",
    "                    path_df.index = pd.to_datetime(path_df.index, unit='s')\n",
    "                    tmp_filtered_path_df = path_df.resample('2S').max().ffill()\n",
    "                    if (\"_L\" not in current):\n",
    "                        norm = 0\n",
    "                        while (len(tmp_filtered_path_df) < 300):\n",
    "\n",
    "                            new_row = pd.DataFrame(columns=['time','len'])\n",
    "                            new_row.loc[0,'time'] = 0+ norm/1000.0\n",
    "                            new_row.loc[0,'len'] = 0.00001\n",
    "                            new_row = new_row.set_index('time')\n",
    "                            new_row.index = pd.to_datetime(new_row.index, unit='s')\n",
    "                            norm += 1\n",
    "                            tmp_filtered_path_df = pd.concat([new_row, tmp_filtered_path_df])\n",
    "                        filtered_path_df.append(tmp_filtered_path_df[-300:]['len'])\n",
    "                    else:\n",
    "                        norm = 0\n",
    "                        while (len(tmp_filtered_path_df) < 450):\n",
    "\n",
    "                            new_row = pd.DataFrame(columns=['time','len'])\n",
    "                            new_row.loc[0,'time'] = 0 + norm/1000.0\n",
    "                            norm += 1\n",
    "                            new_row.loc[0,'len'] = 0.00001\n",
    "                            new_row = new_row.set_index('time')\n",
    "                            new_row.index = pd.to_datetime(new_row.index, unit='s')\n",
    "                            tmp_filtered_path_df = pd.concat([new_row, tmp_filtered_path_df])\n",
    "                            \n",
    "                        filtered_path_df.append(tmp_filtered_path_df[-450:]['len'])\n",
    "\n",
    "                    ate = open(os.path.join(current,'ate.txt'), 'r')\n",
    "                    for l in ate:\n",
    "                        if 'rmse' in l:\n",
    "                            tot_ATE.append(float(l.split()[-2]))\n",
    "                    \n",
    "                    rpe = open(os.path.join(current,'rpe.txt'), 'r')\n",
    "                    for l in rpe:\n",
    "                        if 'translational' in l and 'rmse' in l:\n",
    "                            tot_TRPE.append(float(l.split()[-2]))\n",
    "                        elif 'rotational' in l and 'rmse' in l:\n",
    "                            tot_RRPE.append(float(l.split()[-2]))\n",
    "                            \n",
    "                    odom = np.load(os.path.join(current,'odom.npy'), allow_pickle=True, encoding=\"latin1\")\n",
    "                    for k in range(len(odom.item()['time'])):\n",
    "                        if (odom.item()['time'][-1].to_sec() - odom.item()['time'][k].to_sec() <= 603):\n",
    "                            m = np.array(odom.item()['cov'][k]).reshape((6,6))\n",
    "                            eigen = np.linalg.eig(m)\n",
    "                            pose_cov_vec.append(np.exp(1/6.0 * np.sum(np.log(eigen[0]))).real)\n",
    "                # collect map scores from general_results file\n",
    "                for l in general_results:\n",
    "                    if 'map_score' in l:\n",
    "                        tot_score.append(float(l.split()[-1]))\n",
    "                    if 'BAC score' in l:\n",
    "                        tot_BAC.append(float(l.split()[-1]))\n",
    "                    if 'correct_free_ratio' in l:\n",
    "                        tot_BAC_free.append(float(l.split()[-1]))\n",
    "                    if 'correct_occ_ratio' in l:\n",
    "                        tot_BAC_occ.append(float(l.split()[-1]))\n",
    "\n",
    "                # collect link variances of the graph\n",
    "                # for l in poses:\n",
    "                #     if 'EDGE' in l:\n",
    "                #         s = l.split()\n",
    "                #         m = np.array([[float(s[-6]), float(s[-5]), float(s[-4])],\n",
    "                #                       [float(s[-5]), float(s[-3]), float(s[-2])],\n",
    "                #                       [float(s[-4]), float(s[-2]), float(s[-1])]]\n",
    "                #                     )\n",
    "                #         cov = np.linalg.inv(m)\n",
    "                #         eigen = np.linalg.eig(cov)\n",
    "                #         cov_vec.append(np.exp(1/3.0 * np.sum(np.log(eigen[0]))).real)\n",
    "                        #cov_vec.append(np.linalg.det(cov))\n",
    "        if(len(tot_score)>0):\n",
    "            wheel_over_path = list(map(truediv, wheel_tot, path_len))\n",
    "            all_BAC.append(bac_manually)\n",
    "            all_ATE.append(tot_ATE)\n",
    "            all_ENT.append(entropy_bucketed)\n",
    "            all_PtL.append(path_len)\n",
    "            all_PmL.append(list(map(truediv, tot_lc, path_len)))\n",
    "\n",
    "            if (saving_results):\n",
    "                df.loc['BAC',i.split('/')[-1]] = 'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(bac_manually),3),round(np.std(bac_manually),3))\n",
    "                df.loc['BAC_new',i.split('/')[-1]] = 'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(bac_manually_2),3),round(np.std(bac_manually_2),3))\n",
    "                df.loc['per_m_wheel',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(wheel_over_path),3), + round(np.std(wheel_over_path),3))\n",
    "            else:\n",
    "                df.loc['per_m_wheel',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(wheel_over_path),3), + round(np.std(wheel_over_path),3))\n",
    "                df.loc['BAC',i.split('/')[-1]] = 'mean: {:.3f} std: {:.3f}'.format(round(np.mean(bac_manually),3),round(np.std(bac_manually),3))\n",
    "                df.loc['BAC_new',i.split('/')[-1]] = 'mean: {:.3f} std: {:.3f}'.format(round(np.mean(bac_manually_2),3),round(np.std(bac_manually_2),3))\n",
    "            if ate:\n",
    "                if (saving_results):\n",
    "                    df.loc['final_entropy_norm',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(entropy_bucketed, axis = 0)[-1],3),round(np.std(entropy_bucketed, axis = 0)[-1],3))\n",
    "                    df.loc['ATE RMSE',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(tot_ATE),3),round(np.std(tot_ATE),3))\n",
    "                else:\n",
    "                    df.loc['ATE RMSE',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(tot_ATE),3),round(np.std(tot_ATE),3))\n",
    "                    df.loc['final_entropy_norm',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(entropy_bucketed, axis = 0)[-1],3),round(np.std(entropy_bucketed, axis = 0)[-1],3))\n",
    "                df.loc['path_len',i.split('/')[-1]] =  'mean: $' + str(np.mean(path_len))[:5] + '$ ' + ' std: $' + str(np.std(path_len))[:5]+'$'\n",
    "                lc_over_path = list(map(truediv, tot_lc, path_len))\n",
    "                if (saving_results):\n",
    "                    df.loc['Loops per m',i.split('/')[-1]] =  'mean: ${:.3f}$ std: ${:.3f}$'.format(round(np.mean(lc_over_path),3),round(np.std(lc_over_path),3))\n",
    "                else:\n",
    "                    df.loc['Loops per m',i.split('/')[-1]] =  'mean: {:.3f} std: {:.3f}'.format(round(np.mean(lc_over_path),3),round(np.std(lc_over_path),3))\n",
    "                \n",
    "            panda = pd.DataFrame(columns=['time','norm_entropy','area','path_len','exp'])\n",
    "            cnt_exp = 0\n",
    "            for tries in range(len(entropy_bucketed)):\n",
    "                for result in range(len(entropy_bucketed[tries])):\n",
    "                    panda.loc[cnt_exp,'norm_entropy'] = entropy_bucketed[tries][result]\n",
    "                    panda.loc[cnt_exp,'time'] = result*2\n",
    "                    if ('E2' in current):\n",
    "                        panda.loc[cnt_exp,'area'] = area_bucketed[tries][result]/204.0 #area_bucketed[tries][result]\n",
    "                    else:\n",
    "                        panda.loc[cnt_exp,'area'] = area_bucketed[tries][result]/183.0 #area_bucketed[tries][result]\n",
    "                    panda.loc[cnt_exp,'path_len'] = filtered_path_df[tries][result]\n",
    "                    cnt_exp += 1\n",
    "            panda.norm_entropy = panda.norm_entropy.astype(float)\n",
    "            panda.path_len = panda.path_len.astype(float)\n",
    "            panda.area = panda.area.astype(float)\n",
    "            lab_ = i.split('/')[-1].replace('E1_','').replace('E2_','')\n",
    "            lab_ = \"$\" + lab_[0:2] + \"{\"+ lab_[2:] +\"}\" +\"$\"\n",
    "            \n",
    "            sns.lineplot(data=panda, x='time', ci=\"sd\", y=\"area\", ax=ax_map, color=color[cnt], label=lab_)\n",
    "            sns.lineplot(data=panda, x='time', ci=\"sd\", y=\"path_len\", ax=ax_map_split, color=color[cnt])\n",
    "\n",
    "            if ('E2' in current):\n",
    "                lab = i.split('/')[-1].replace('E2_','')\n",
    "            else:\n",
    "                lab = i.split('/')[-1].replace('E1_','')\n",
    "            lab = \"$\" + lab[0:2] + \"{\"+ lab[2:] +\"}\" +\"$\"\n",
    "            print(lab)\n",
    "            if ('E2' in current):\n",
    "                atyy = np.array(area_bucketed).astype(np.float32)/204.0\n",
    "                _n_err = -1.96*np.std(atyy, axis = 0)/np.sqrt(atyy.shape[0]) + np.mean(area_bucketed, axis = 0).astype(np.float32)/204.0\n",
    "                _p_err = +1.96*np.std(atyy, axis = 0)/np.sqrt(atyy.shape[0]) + np.mean(area_bucketed, axis = 0).astype(np.float32)/204.0\n",
    "                \n",
    "                ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(area_bucketed, axis = 0)/204.0,color=color[cnt], label=lab)\n",
    "                ax_exp.fill_between(x=np.mean(filtered_path_df,axis=0).astype(np.float32), y1=_n_err, y2=_p_err ,color=color[cnt], alpha = 0.2)\n",
    "            else:\n",
    "\n",
    "                atyy = np.array(area_bucketed).astype(np.float32)/183.0\n",
    "                _n_err = -1.96*np.std(atyy, axis = 0)/np.sqrt(atyy.shape[0]) + np.mean(area_bucketed, axis = 0).astype(np.float32)/183.0\n",
    "                _p_err = +1.96*np.std(atyy, axis = 0)/np.sqrt(atyy.shape[0]) + np.mean(area_bucketed, axis = 0).astype(np.float32)/183.0\n",
    "\n",
    "                ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(area_bucketed, axis = 0)/183.0,color=color[cnt], label=lab)\n",
    "                ax_exp.fill_between(x=np.mean(filtered_path_df,axis=0).astype(np.float32), y1=_n_err, y2=_p_err ,color=color[cnt], alpha = 0.2)\n",
    "            \n",
    "            etty = np.array(entropy_bucketed).astype(np.float32)\n",
    "            _n_err = -1.96*np.std(etty, axis = 0)/np.sqrt(etty.shape[0]) + np.mean(entropy_bucketed, axis = 0).astype(np.float32)\n",
    "            _p_err = +1.96*np.std(etty, axis = 0)/np.sqrt(etty.shape[0]) + np.mean(entropy_bucketed, axis = 0).astype(np.float32)    \n",
    "            ax_exp.plot(np.mean(filtered_path_df,axis=0), np.mean(entropy_bucketed, axis=0),color=color[cnt])\n",
    "            ax_exp.fill_between(x=np.mean(filtered_path_df,axis=0).astype(np.float32), y1=_n_err, y2=_p_err ,color=color[cnt], alpha = 0.2)\n",
    "\n",
    "            ax_exp.legend(ncol=2)\n",
    "            \n",
    "#             ax_ent.legend()\n",
    "\n",
    "            ax_map.legend(ncol=2)\n",
    "\n",
    "#             ax_area.legend()\n",
    "\n",
    "            # plot CDF\n",
    "#             edges = compute_bin_size(cov_vec)\n",
    "#             n, bins, patches = ax1.hist(cov_vec, edges, cumulative=True, density=True, histtype='step', label=i.split('/')[-1], color=color[cnt])\n",
    "#             patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "#             #ax1.plot(bins, n,  label=columns[cnt],c=np.random.rand(3,) )\n",
    "#             if ate:\n",
    "#                 edges = com',pute_bin_size(pose_cov_vec)\n",
    "#                 n, bins, patches = ax2.hist(pose_cov_vec, edges, cumulative=True, density=True, histtype='step', label=i.split('/')[-1], color=color[cnt])\n",
    "#                 patches[0].set_xy(patches[0].get_xy()[:-1])\n",
    "                #ax2.plot(bins[1:], n,  ,c=np.random.rand(3,) )\n",
    "            cnt += 1\n",
    "#             print(tot_area)\n",
    "            print(len(tot_lc))\n",
    "            print('---------')\n",
    "    \n",
    "#     handles, labels = ax1.get_legend_handles_labels()\n",
    "#     new_handles = [lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "#     ax1.legend(handles=new_handles, labels=labels)\n",
    "\n",
    "#     if ate:\n",
    "#         handles, labels = ax2.get_legend_handles_labels()\n",
    "#         new_handles = [lines.Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
    "#         ax2.legend(handles=new_handles, labels=labels)\n",
    "#     fig1.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "#     fig1.show()\n",
    "    \n",
    "#     fig_map.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "    fig_map.show() \n",
    "#     fig_exp.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "    fig_exp.show()        \n",
    "    \n",
    "    separator = \"-\"\n",
    "    if (saving_results):\n",
    "        fig_map.savefig('map-{}-{}-{}.eps'.format('E1' if 'E1' in folder else 'E2',separator.join(columns),separator.join(tests)),format='eps',bbox_inches='tight')\n",
    "        fig_exp.savefig('exp-{}-{}-{}.eps'.format('E1' if 'E1' in folder else 'E2',separator.join(columns),separator.join(tests)),format='eps',bbox_inches='tight')\n",
    "    \n",
    "#     fig_area.suptitle('E1' if 'E1' in folder else 'E2')\n",
    "#     fig_area.show()        \n",
    "    \n",
    "    display(df)\n",
    "    if (saving_results):\n",
    "        print(df.to_latex(index=True, escape=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dc8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcdefaults()\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 13,\n",
    "            }\n",
    "matplotlib.rc('font', **font) \n",
    "\n",
    "columns = ['E1']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "# tests = ['gridmap_transformation_astar', 'gridmap_transformation_dijkstra', 'costmap_default_astar', 'gridmap_default_astar', 'gridmap_default_astar_scaled']\n",
    "# tests = ['gridmap_default_dijkstra', 'gridmap_transformation_dijkstra', 'gridmap_transformation_astar']\n",
    "tests = [\"C_I\", \"C_costmap\", \"C_gridmap\"]\n",
    "folder = '/media/statordyna/Backup Plus/Test/'\n",
    "folder_E2 = os.path.join(folder,'E1')\n",
    "\n",
    "all_BAC = []\n",
    "all_ATE = []\n",
    "all_ENT = []\n",
    "all_PtL = []\n",
    "all_PmL = []\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "# a = get_data(folder_E2, columns, tests, df_2_,0,2, True, True)\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,10, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edf194cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tab_gcpr = [[], [], []]\n",
    "map_s = tests\n",
    "for m in range(len(map_s)):\n",
    "    print(m)\n",
    "    for i in range(10):\n",
    "        tx = \"\"\n",
    "        # trial_no = i+1\n",
    "        # tx+=str(i)+\"&\"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_BAC[m][i], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_ATE[m][i], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_ENT[m][i][-1], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_PtL[m][i], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_PmL[m][i], 3)))\n",
    "        tab_gcpr[m].append(tx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9180644e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "['C_I', 'C_costmap', 'C_gridmap']\n",
      "\n",
      "\n",
      "0 & 0.840 & 0.020 & 0.162 & 77.219 & 7.576 & 0.835 & 0.023 & 0.230 & 75.028 & 7.957 & 0.845 & 0.022 & 0.187 & 78.858 & 7.697 \\\\\n",
      "1 & 0.817 & 0.030 & 0.174 & 80.955 & 6.621 & 0.845 & 0.080 & 0.175 & 85.204 & 5.892 & 0.844 & 0.038 & 0.136 & 87.517 & 6.399 \\\\\n",
      "2 & 0.842 & 0.041 & 0.163 & 79.862 & 6.524 & 0.797 & 0.023 & 0.192 & 80.364 & 6.371 & 0.848 & 0.052 & 0.179 & 85.786 & 7.064 \\\\\n",
      "3 & 0.845 & 0.028 & 0.174 & 72.656 & 8.010 & 0.829 & 0.024 & 0.190 & 83.834 & 6.060 & 0.846 & 0.028 & 0.145 & 84.988 & 6.177 \\\\\n",
      "4 & 0.823 & 0.031 & 0.181 & 77.213 & 7.395 & 0.845 & 0.036 & 0.202 & 76.426 & 5.980 & 0.825 & 0.071 & 0.131 & 74.325 & 5.032 \\\\\n",
      "5 & 0.831 & 0.027 & 0.124 & 76.114 & 6.792 & 0.797 & 0.028 & 0.129 & 52.095 & 2.687 & 0.841 & 0.025 & 0.186 & 83.777 & 6.505 \\\\\n",
      "6 & 0.831 & 0.025 & 0.185 & 73.080 & 6.924 & 0.819 & 0.093 & 0.220 & 81.754 & 5.296 & 0.816 & 0.051 & 0.232 & 81.542 & 6.561 \\\\\n",
      "7 & 0.828 & 0.032 & 0.183 & 77.270 & 7.390 & 0.826 & 0.030 & 0.203 & 84.288 & 6.454 & 0.824 & 0.036 & 0.187 & 83.807 & 6.062 \\\\\n",
      "8 & 0.844 & 0.039 & 0.134 & 78.851 & 6.798 & 0.759 & 0.199 & 0.213 & 78.201 & 4.565 & 0.810 & 0.178 & 0.220 & 78.089 & 5.699 \\\\\n",
      "9 & 0.845 & 0.014 & 0.192 & 74.806 & 8.114 & 0.847 & 0.017 & 0.132 & 84.394 & 7.963 & 0.839 & 0.070 & 0.183 & 87.535 & 6.272 \\\\\n",
      "mean & 0.834 & 0.029 & 0.167 & 76.803 & 7.214 & 0.820 & 0.055 & 0.189 & 78.159 & 5.922 & 0.834 & 0.057 & 0.179 & 82.622 & 6.347 \\\\ \n",
      "std & 0.010 & 0.008 & 0.021 & 2.586 & 0.539 & 0.027 & 0.054 & 0.033 & 9.318 & 1.467 & 0.013 & 0.043 & 0.032 & 4.137 & 0.684 \\\\ \n"
     ]
    }
   ],
   "source": [
    "tab_gcpr = np.array(tab_gcpr)\n",
    "print(tab_gcpr.shape)\n",
    "print(map_s)\n",
    "print(\"\\n\")\n",
    "for i in range(len(tab_gcpr[0])):\n",
    "    print(str(i)+\" & \", end = \"\")\n",
    "    print(tab_gcpr[0][i], end=\" & \")\n",
    "    print(tab_gcpr[2][i], end=\" & \")\n",
    "    print(tab_gcpr[1][i], end=\" \\\\\\\\\\n\")\n",
    "\n",
    "print(\"mean & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_BAC[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ATE[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ENT[0], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PtL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PmL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_BAC[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ATE[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ENT[2], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PtL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PmL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_BAC[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ATE[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ENT[1], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PtL[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PmL[1]), 3)))+\" \\\\\\\\ \") \n",
    "\n",
    "print(\"std & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_BAC[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ATE[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ENT[0], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PtL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PmL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_BAC[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ATE[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ENT[2], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PtL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PmL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_BAC[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ATE[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ENT[1], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PtL[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PmL[1]), 3)))+\" \\\\\\\\ \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcdefaults()\n",
    "matplotlib.rcParams['pdf.fonttype']=42\n",
    "matplotlib.rcParams['ps.fonttype']=42\n",
    "font = {\n",
    "        'weight': 'normal',\n",
    "        'size': 13,\n",
    "            }\n",
    "matplotlib.rc('font', **font) \n",
    "\n",
    "columns = ['E2']#,'S_O']#,'S_O','S_DW_O']#['S',]\n",
    "# tests = ['gridmap_transformation_astar', 'gridmap_transformation_dijkstra', 'costmap_default_astar', 'gridmap_default_astar', 'gridmap_default_astar_scaled']\n",
    "# tests = ['gridmap_default_dijkstra', 'gridmap_transformation_dijkstra', 'gridmap_transformation_astar']\n",
    "tests = [\"C_I\", \"C_costmap\", \"C_gridmap\"]\n",
    "folder = '/media/statordyna/Backup Plus/Test/'\n",
    "folder_E2 = os.path.join(folder,'E2')\n",
    "\n",
    "all_BAC = []\n",
    "all_ATE = []\n",
    "all_ENT = []\n",
    "all_PtL = []\n",
    "all_PmL = []\n",
    "\n",
    "df_2_ = pd.DataFrame()\n",
    "# a = get_data(folder_E2, columns, tests, df_2_,0,2, True, True)\n",
    "a = get_data(folder_E2, columns, tests, df_2_,0,10, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67e44280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tab_gcpr = [[], [], []]\n",
    "map_s = tests\n",
    "for m in range(len(map_s)):\n",
    "    print(m)\n",
    "    for i in range(10):\n",
    "        tx = \"\"\n",
    "        # trial_no = i+1\n",
    "        # tx+=str(i)+\"&\"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_BAC[m][i], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_ATE[m][i], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_ENT[m][i][-1], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_PtL[m][i], 3)))+\" & \"\n",
    "        tx+=str(\"{:.3f}\".format(round(all_PmL[m][i], 3)))\n",
    "        tab_gcpr[m].append(tx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73793d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "['C_I', 'C_costmap', 'C_gridmap']\n",
      "\n",
      "\n",
      "0 & 0.781 & 0.020 & 0.080 & 84.700 & 6.765 & 0.817 & 0.033 & 0.252 & 86.457 & 6.084 & 0.772 & 0.055 & 0.160 & 79.041 & 4.884 \\\\\n",
      "1 & 0.779 & 0.026 & 0.177 & 72.734 & 5.692 & 0.820 & 0.109 & 0.220 & 90.004 & 4.989 & 0.778 & 0.027 & 0.160 & 89.807 & 5.723 \\\\\n",
      "2 & 0.811 & 0.075 & 0.262 & 77.507 & 5.987 & 0.744 & 0.050 & 0.199 & 84.777 & 4.966 & 0.805 & 0.037 & 0.164 & 92.816 & 6.281 \\\\\n",
      "3 & 0.833 & 0.020 & 0.084 & 86.342 & 6.440 & 0.740 & 0.062 & 0.155 & 89.814 & 5.656 & 0.800 & 0.080 & 0.244 & 83.942 & 5.492 \\\\\n",
      "4 & 0.808 & 0.022 & 0.133 & 83.184 & 6.323 & 0.771 & 0.332 & 0.245 & 87.955 & 4.821 & 0.763 & 0.022 & 0.256 & 86.696 & 6.875 \\\\\n",
      "5 & 0.807 & 0.043 & 0.195 & 72.289 & 6.778 & 0.770 & 0.153 & 0.215 & 89.608 & 4.308 & 0.781 & 0.023 & 0.172 & 86.625 & 6.280 \\\\\n",
      "6 & 0.772 & 0.022 & 0.193 & 81.013 & 6.505 & 0.816 & 0.021 & 0.166 & 92.556 & 6.266 & 0.781 & 0.017 & 0.191 & 87.173 & 6.711 \\\\\n",
      "7 & 0.805 & 0.048 & 0.244 & 81.786 & 6.016 & 0.809 & 0.037 & 0.229 & 89.579 & 5.783 & 0.765 & 0.032 & 0.180 & 90.606 & 5.585 \\\\\n",
      "8 & 0.800 & 0.021 & 0.150 & 73.760 & 5.437 & 0.790 & 0.035 & 0.084 & 90.388 & 6.461 & 0.808 & 0.019 & 0.184 & 85.889 & 6.415 \\\\\n",
      "9 & 0.761 & 0.026 & 0.178 & 79.546 & 6.449 & 0.783 & 0.316 & 0.245 & 91.113 & 4.643 & 0.786 & 0.099 & 0.249 & 74.407 & 6.008 \\\\\n",
      "mean & 0.796 & 0.032 & 0.170 & 79.286 & 6.239 & 0.786 & 0.115 & 0.201 & 89.225 & 5.398 & 0.784 & 0.041 & 0.196 & 85.700 & 6.025 \\\\ \n",
      "std & 0.021 & 0.017 & 0.057 & 4.787 & 0.423 & 0.028 & 0.112 & 0.050 & 2.152 & 0.708 & 0.015 & 0.027 & 0.036 & 5.202 & 0.579 \\\\ \n"
     ]
    }
   ],
   "source": [
    "tab_gcpr = np.array(tab_gcpr)\n",
    "print(tab_gcpr.shape)\n",
    "print(map_s)\n",
    "print(\"\\n\")\n",
    "for i in range(len(tab_gcpr[0])):\n",
    "    print(str(i)+\" & \", end = \"\")\n",
    "    print(tab_gcpr[0][i], end=\" & \")\n",
    "    print(tab_gcpr[2][i], end=\" & \")\n",
    "    print(tab_gcpr[1][i], end=\" \\\\\\\\\\n\")\n",
    "\n",
    "print(\"mean & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_BAC[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ATE[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ENT[0], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PtL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PmL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_BAC[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ATE[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ENT[2], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PtL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PmL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_BAC[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ATE[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_ENT[1], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PtL[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.mean(all_PmL[1]), 3)))+\" \\\\\\\\ \") \n",
    "\n",
    "print(\"std & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_BAC[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ATE[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ENT[0], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PtL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PmL[0]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_BAC[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ATE[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ENT[2], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PtL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PmL[2]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_BAC[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ATE[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_ENT[1], axis=0)[-1], 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PtL[1]), 3)))+\" & \", end=\"\")\n",
    "print((\"{:.3f}\".format(round(np.std(all_PmL[1]), 3)))+\" \\\\\\\\ \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b6b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
